#!/usr/bin/env ruby
#MIT License
#Copyright (c) 2017-2018 phR0ze
#
#Permission is hereby granted, free of charge, to any person obtaining a copy
#of this software and associated documentation files (the "Software"), to deal
#in the Software without restriction, including without limitation the rights
#to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
#copies of the Software, and to permit persons to whom the Software is
#furnished to do so, subject to the following conditions:
#
#The above copyright notice and this permission notice shall be included in all
#copies or substantial portions of the Software.
#
#THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
#IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
#FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
#AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
#LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
#OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
#SOFTWARE.

require 'fileutils'             # advanced file utils: FileUtils
require 'open3'                 # Better system commands
require 'ostruct'               # OpenStruct
require 'rubygems/package'      # tar
require 'yaml'                  # YAML

# Gems that need to be installed
begin
  require 'nub'
rescue Exception => e
  puts("Error: missing package '#{e.message.split(' ').last.sub('/', '-')}'")
  !puts("Error: install missing packages with 'bundle install --system") and exit
end

class Kubinator

  # Initialize
  # ==== Attributes
  def initialize()

    # Minimum versions
    @helmver = '2.9.1'
    @vagrantver = '1.8.1'
    @virtualboxver = '5.0.32'

    # Semi-static variables
    @user = 'vagrant'
    @pass = 'vagrant'
    @host = 'k8snode'
    @netname = 'vboxnet0'
    @netip = '192.168.56.1'
    @netmask = '255.255.255.0'
  end

  # Deploy vagrant node/s
  # @param nodes [Array] of last octet ips for vms
  # @param cpu [Int] count to use for vms
  # @param ram [Int] amount of ram in mb for vms
  # @param box [String] version for vms
  def deploy(nodes:nil, cpu:nil, ram:nil, box:nil)
    puts(":: Deploying vagrant vms...".colorize(:light_yellow))

    # Validate vagrant environment
    #---------------------------------------------------------------------------
    puts(":: Validating environment tooling".colorize(:cyan))

    # Check that vagrant is installed and the correct version
    !puts("Ensure 'vagrant' is installed and on the $PATH".colorize(:red)) and
      exit unless FileUtils.exec?('vagrant')
    vagrantver = `vagrant --version`[/\d+\.\d+\.\d+/]
    !puts("Vagrant needs to be version #{@vagrantver} or higher".colorize(:red)) and
      exit unless Gem::Version.new(vagrantver) >= Gem::Version.new(@vagrantver)
    puts("Found Vagrant: #{vagrantver}".colorize(:green))

    # Check that vagrant/virtualbox are on the path and clean previously deployed VMs
    !puts("Ensure 'virtualbox' is installed and on the $PATH".colorize(:red)) and
      exit unless FileUtils.exec?('vboxmanage')
    virtualboxver = `vboxmanage --version`[/\d+\.\d+\.\d+/]
    !puts("Virtualbox needs to be version #{@virtualboxver} or higher".colorize(:red)) and
      exit unless Gem::Version.new(virtualboxver) >= Gem::Version.new(@virtualboxver)
    puts("Found Virtualbox: #{virtualboxver}".colorize(:green))

    # Ensure host-only network exists
    #---------------------------------------------------------------------------
    puts(":: Ensure host-only networking exists".colorize(:cyan))
    config_network = "vboxmanage hostonlyif ipconfig #{@netname} -ip #{@netip} -netmask #{@netmask}"
    if not Sys.exec(config_network, die:false)
      Sys.exec("vboxmanage hostonlyif create")
      Sys.exec(config_network)
    end

    # Generate vagrant node parameters
    #---------------------------------------------------------------------------
    specs = []
    (nodes || (10..12)).each{|node|
      spec = {
        host: "#{@host}#{node}",
        ip: "#{@netip[0..-2]}#{node}/24",
        cpus: cpu || 2,
        ram: ram || 2048,
        vram: 8,
        net: @netname,
        v3d: 'off',
        proxy: Net.proxy.http,
        no_proxy: Net.proxy.no,
        ipv6: nil
      }
      specs << spec
      puts("Generating node: #{spec.to_s}".colorize(:cyan))
    }
    exit

    # Read in the template file and write out with ips
    FileUtils.cp('vagrant.tpl', 'Vagrantfile')
    vars = OpenStruct.new()
    vars.boxver = box
    vars.nodes = specs.map{|x| '  ' + x.to_s} * ",\n"
    resolve('Vagrantfile', vars)

    # Initialize vagrant box
    #-----------------------------------------------------------------------
    puts("Initializing vagrant box/s '#{box}'".colorize(:cyan))
    sys("vagrant up")
    sys("vagrant reload")
  end

  # Create Kubernetes cluster from given nodes
  # ==== Attributes
  # * +ips+ - list of node ips to cluster
  # * +init+ - initialize the main node
  # * +join+ - join all the slaves to the cluster
  # * +dashboard+ - install the dashboard
  # * +registry+ - private registry to authorize
  # * +helm+ - install helm
  def deploy_cluster(ips, init:nil, join:nil, dashboard:nil, helm:nil, registry:nil)
    puts("Deploying Kubernetes Cluster".colorize(:yellow))
    puts("Nodes: #{ips * ', '}".colorize(:yellow))
    puts("#{'-' * 80}".colorize(:cyan))
    all = ![init, join].any?

    token = 'c74b2e.897db207c8e26de2'

    # Determine k8s version
    #---------------------------------------------------------------------------
    k8sver = nil
    Net::SSH.start(ips.first, @user, password:@pass, paranoid:false) do |ssh|
      k8sver = ssh.exec!("sudo pacman -Q kubernetes-kubectl")[/\d+\.\d+\.\d+/]
      puts("k8sver = #{k8sver}")
    end

    # Validate the environment
    #---------------------------------------------------------------------------
    puts("Validating the environment for the correct k8s tooling".colorize(:cyan))
    !puts("Please unset uppercase proxy variables (HTTP_PROXY, HTTPS_PROXY, and NO_PROXY)".colorize(:red)) and
      exit unless not [ENV['HTTP_PROXY'], ENV['HTTPS_PROXY'], ENV['NO_PROXY']].any?

    !puts("Please unset KUBECONFIG. We will be using 'contexts' instead".colorize(:red)) and
      exit unless not ENV['KUBECONFIG']

    # Check environment variables
    !puts("Ensure 'no_proxy' includes your node ips #{ips * ','}".colorize(:red)) and
      exit unless not @proxy or ips.each{|x| ENV['no_proxy'].include?(x)}
    puts("$no_proxy configured correctly".colorize(:green)) if @proxy

    # Validate that kubectl is installed on the path and a supported version
    !puts("Ensure 'kubectl' is installed and on the path".colorize(:red)) and
      exit unless find_executable('kubectl')
    stdout, stderr, status = Open3.capture3("kubectl version")
    kubectlver = stdout[/GitVersion:\"v\d+\.\d+\.\d+/].split('v').last
    !puts("Kubectl needs to be version #{k8sver} or higher".colorize(:red)) and
      exit unless Gem::Version.new(kubectlver) >= Gem::Version.new(k8sver)
    puts("Kubectl: #{kubectlver}".colorize(:green))

    # Validate that helm is installed on the path and a supported version
    !puts("Ensure 'helm' is installed and on the path".colorize(:red)) and
      exit unless find_executable('helm')
    helmver = `helm version --client`[/\d+\.\d+\.\d+/]
    !puts("Helm needs to be version #{@helmver} or higher".colorize(:red)) and
      exit unless Gem::Version.new(helmver) >= Gem::Version.new(@helmver)
    puts("Helm: #{helmver}".colorize(:green))

    # Clear K8s cache
    #---------------------------------------------------------------------------
    cache_dir = File.expand_path("~/.kube")
    FileUtils.rm_rf(cache_dir)
    FileUtils.mkdir_p(cache_dir)

    # (Idempotent) Configure nodes for clustering
    #---------------------------------------------------------------------------
    threads = []
    ips.each{|ip| threads << Thread.new{
      Net::SSH.start(ip, @user, password:@pass, paranoid:false) do |ssh|

        # Configure journald for persistent storage
        journald_conf = '/etc/systemd/journald.conf'
        if not ssh.exec!("cat #{journald_conf}").include?('persistent')
          ssh.exec!("sudo sed -i -e 's/.*\\(Storage=\\).*/\\1persistent/' #{journald_conf}")
          ssh.exec!("sudo systemctl restart systemd-journald")
          puts("#{ip}: Configured node journald...done".colorize(:cyan))
        else
          puts("#{ip}: Configured node journald...skipped".colorize(:cyan))
        end

        # Configure kubelet/k8s api server
        # https://kubernetes.io/docs/admin/kubeadm
        kubeadm_conf = '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf'
        if not ssh.exec!("cat #{kubeadm_conf}").include?(ip)
          api_args = [
            "--node-ip=#{ip}",
            "--hostname-override=#{ip}"
            # Else: /etc/kubernetes/manifests/kube-apiserver.yaml
            # - --runtime-config=batch/v2alpha1=true
            # To enable jobs: kubeadmin init --config /etc/kubernetes/kubeadm.conf
            #"--runtime-config=batch\\/v2alpha1=true"
          ]
          extra_args = "Environment=\"KUBELET_EXTRA_ARGS=#{api_args * ' '}\"\\n"
          ssh.exec!("sudo sed -i -e 's/\\(ExecStart=\\)$/#{extra_args}\\1/' #{kubeadm_conf}")
          ssh.exec!("sudo systemctl daemon-reload")
          ssh.exec!("sudo systemctl restart kubelet")
          puts("#{ip}: Configured Kubelet private network ip...done".colorize(:cyan))
        else
          puts("#{ip}: Configured Kubelet private network ip...skipped".colorize(:cyan))
        end

        # Configure kernel for Elasticsearch
        sysctl_conf = '/etc/sysctl.d/10-cyberlinux.conf'
        if not ssh.exec!("cat #{sysctl_conf}").include?('max_map_count')
          ssh.exec!("sudo bash -c 'echo \"vm.max_map_count = 262144\" >> #{sysctl_conf}'")
          ssh.exec!("sudo sysctl -w vm.max_map_count=262144")
        else
          puts("#{ip}: Configured kernel params...skipped".colorize(:cyan))
        end

        # Set docker storage-driver from overlay2 to overlay
        driver = "overlay"
        daemon_conf = '/etc/docker/daemon.json'
        if not ssh.exec!("[ -e #{daemon_conf} ] && echo 'exists'").include?('exists')

          # Configure docker storage-driver
          docker_conf = "{\\\"storage-driver\\\":\\\"#{driver}\\\"}"
          ssh.exec!("sudo bash -c 'mkdir -p /etc/docker/'")
          ssh.exec!("sudo bash -c 'echo \"#{docker_conf}\" > /etc/docker/daemon.json'")

          ssh.exec!("sudo systemctl daemon-reload")
          ssh.exec!("sudo systemctl restart docker")
          puts("#{ip}: Configured Docker storage-driver...done".colorize(:cyan))
        else
          puts("#{ip}: Configured Docker storage-driver...skipped".colorize(:cyan))
        end

        # Optionally - configure private registry
        if registry
          docker_opts = [ "--registry-mirror=http://#{registry}", "--insecure-registry #{registry}" ]
          override_conf = '/etc/systemd/system/docker.service.d/override.conf'
          if not ssh.exec!("[ -e #{override_conf} ] && echo 'exists'").include?('exists')

            # Configure kubernetes for private registry
            docker_conf = "{\\\"auths\\\":{\\\"#{registry}\\\":{\\\"auth\\\":\\\"YW5vbnltb3VzOmFub255bW91cw==\\\"}}}"
            ssh.exec!("sudo bash -c 'mkdir -p /root/.docker'")
            ssh.exec!("sudo bash -c 'echo \"#{docker_conf}\" > /root/.docker/config.json'")

            # Configure docker for private registry
            ssh.exec!("sudo bash -c 'echo \"[Service]\" > #{override_conf}'")
            ssh.exec!("sudo bash -c 'echo \"ExecStart=\" >> #{override_conf}'")
            ssh.exec!("sudo bash -c 'echo \"ExecStart=/usr/bin/dockerd #{docker_opts * ' '} -H fd://\" >> #{override_conf}'")
            ssh.exec!("sudo systemctl daemon-reload")
            ssh.exec!("sudo systemctl restart docker")
            puts("#{ip}: Configured Docker overrides...done".colorize(:cyan))
          else
            puts("#{ip}: Configured Docker overrides...skipped".colorize(:cyan))
          end

        end
      end
    }}
    threads.each{|x| x.join}

    # (Idempotent) Initialize master node
    # https://kubernetes.io/docs/getting-started-guides/kubeadm
    #---------------------------------------------------------------------------
    if all or init
      Net::SSH.start(ips.first, @user, password:@pass, paranoid:false) do |ssh|
        if not ssh.exec!("docker ps").include?("google")

          # Initialize master - pod-network injects the --cluster-cidr into kube-proxy
          puts("#{ips.first}: Initialize Master node...".colorize(:cyan))
          cmd = "sudo kubeadm init --token=#{token} --kubernetes-version=v#{k8sver} --apiserver-advertise-address=#{ips.first}"
          ssh.exec!(cmd){|c,s,o|puts(o)}
          (1..10).each{|i| puts("Waiting for k8s networking to quiesce..#{i}"); sleep(1)}
          ssh.exec!("mkdir -p ~/.kube")
          ssh.exec!("sudo mkdir -p /root/.kube")
          ssh.exec!("sudo cp /etc/kubernetes/admin.conf /root/.kube/config")
          ssh.exec!("sudo cp /etc/kubernetes/admin.conf ~/.kube/config")
          ssh.exec!("sudo chown $(id -u):$(id -g) ~/.kube/config")
          ssh.exec!("kubectl config use-context kubernetes-admin@kubernetes")
          ssh.exec!("sudo kubectl config use-context kubernetes-admin@kubernetes")

          # Disable RBAC for now (will wait until needed)
          # Bottom of https://kubernetes.io/docs/admin/authorization/rbac
          ssh.exec!("kubectl create clusterrolebinding permissive-binding --clusterrole=cluster-admin --user=admin --user=kubelet --group=system:serviceaccounts"){|c,s,o|puts(o)}

          # Patch kube-proxy for dns resolution using --proxy-mode=iptables
          # https://github.com/kubernetes/kubernetes/issues/18934
          get_kube_proxy = "kubectl -n kube-system get ds kube-proxy -o json"
          mod_kube_proxy = " | jq '.spec.template.spec.containers[0].command |= .+ [\"--proxy-mode=iptables\"]'"
          mod_kube_proxy += " | jq '.spec.template.spec.containers[0].command |= .+ [\"--cluster-cidr=10.32.0.0/12\"]'"
          set_kube_proxy = " | kubectl apply -f -"
          ssh.exec!("#{get_kube_proxy} #{mod_kube_proxy} #{set_kube_proxy}"){|c,s,o|puts(o)}
          ssh.exec!("kubectl -n kube-system delete po -l 'k8s-app=kube-proxy'"){|c,s,o|puts(o)}

          # Taint master node to allow pods to be scheduled on it
          # Check current taints in Tains section: kubectl describe node
          # Note: this needs to be done before deploying flannel
          puts("Tainting the master node to allow pods to be scheduled on it".colorize(:cyan))
          ssh.exec!("kubectl taint nodes --all node-role.kubernetes.io/master-"){|c,s,o|puts(o)}

          # Install Pod networking
          # Note: kube-dns will be pending until the pod networking is installed
          puts("Installing weave networking".colorize(:cyan))
          ssh.exec!("#{@proxy_export}kubectl apply -f https://git.io/weave-kube-1.6"){|c,s,o|puts(o)}
        end
      end
    end

    # (Idempotent) Configure slaves to join cluster
    #---------------------------------------------------------------------------
    if all or join
      nodes_to_join = []
      Net::SSH.start(ips.first, @user, password:@pass, paranoid:false) do |ssh|
        output = getpods(ssh:ssh)
        nodes_to_join = ips.select{|x| !output.include?(x)}
      end
      nodes_to_join.each do |ip|
        Net::SSH.start(ip, @user, password:@pass, paranoid:false) do |ssh|
          puts("#{ip}: Joining cluster".colorize(:cyan))
          ssh.exec!("sudo kubeadm join --token=#{token} #{ips.first}:6443"){|c,s,o|puts(o)}
        end
      end
    end

    # (Idempotent) Install dashboard and helm
    #---------------------------------------------------------------------------
    Net::SSH.start(ips.first, @user, password:@pass, paranoid:false) do |ssh|

      # Install dashboard
      if all or dashboard
        if not getpods(pod:'dashboard', ssh:ssh)
          puts("#{ips.first}: Installing dashboard".colorize(:cyan))
          url = "https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml"
          puts(ssh.exec!("kubectl create -f #{url}"))
        else
          puts("#{ips.first}: Installing dashboard...skipped".colorize(:cyan))
        end
      end

      # Initialize/update helm
      if all or helm
        if not getpods(pod:'tiller', ssh:ssh)
          puts("#{ips.first}: Initializing helm".colorize(:cyan))
          ssh.exec!("#{@proxy_export}helm init"){|c,s,o|puts(o)}
          ssh.exec!("#{@proxy_export}helm repo update"){|c,s,o|puts(o)}
        else
          puts("#{ips.first}: Initializing/updating helm...skipped".colorize(:cyan))
        end
      end
    end

    # Configure local kubectl
    kube_conf = File.join(cache_dir, "config")
    FileUtils.rm_rf(kube_conf)
    Net::SCP.download!(ips.first, @user, '.kube/config', kube_conf, ssh:{paranoid: false, password: @pass})
    sys("kubectl config use-context kubernetes-admin@kubernetes")
    puts("#{'-' * 80}".colorize(:cyan))
    sys("kubectl get po --all-namespaces -o wide")
    puts("#{'-' * 80}".colorize(:cyan))
    puts("Your cluster will need a few minutes to be fully operational".colorize(:cyan))
  end

  # Wait for the given pod to be ready
  # Blocks until service is ready
  # ==== Attributes
  # * +pod+ - pod by name to wait for
  def podready!(pod)
    details = getpods(pod:pod)
    status = details ? details[3] : ""

    # Skip wait if pod already running
    if status.include?('Running')
      puts("Waiting for '#{pod}' to be ready - Running".colorize(:cyan))

    # Wait for pod to be ready
    else
      ready = 0
      until ready > 1
        !puts("Waiting for '#{pod}' to be ready - #{status}".colorize(:cyan)) and sleep(10)
        details = getpods(pod:pod)
        status = details ? details[3] : ""

        ready += 1 if status.include?('Running')
      end
    end
  end

  # Get pod details
  # ==== Attributes
  # * +pod+ - pod by name to pull details for
  # * +ssh+ - optionaly use ssh connection to use
  def getpods(pod:nil, ssh:nil)
    details = ssh ? ssh.exec!("kubectl get pod --all-namespaces -o wide") :
      `kubectl get pod --all-namespaces -o wide`
    if pod
      details = details.split("\n").find{|x| x.include?(pod)}
      details = details.split(' ').map{|x| x.strip} if details
      return details
    end
    return details
  end

  # Parse the node ips out of the Vagrantfile
  # ==== Attributes
  # * +returns+ - list of node ips
  def getnodeips()
    ips = []

    pattern = @netip.split('.')[0..-2] * '.'
    File.open('Vagrantfile', 'r') do |f|
      f.readlines.each{|x|
        if x =~ /#{pattern}.*/
          ips << x[/#{pattern}\.\d+/]
        end
      }
    end

    return ips
  end
end

#-------------------------------------------------------------------------------
# Main entry point
#-------------------------------------------------------------------------------
if __FILE__ == $0
  app = 'kubinator'
  kubinator = Kubinator.new
  version = FileUtils.version('.gemspec', /\s*spec\.version\s*=.*(\d+\.\d+\.\d+).*/)
  examples = "Deploy: ./#{app} deploy\n".colorize(:green)
  examples += "Deploy custom: ./#{app} deploy --ips=10,11,12 --cpu=2 --ram=4096\n".colorize(:green)

  cmdr = Commander.new(app:app, version:version, examples:examples)
  cmdr.add('deploy', 'Deploy Vagrant nodes with desired configuration', nodes:[
    Option.new('--nodes=NODES', 'List of last octet IPs (e.g. 10,11,2)', type:Array),
    Option.new('--cpu=CPU', 'Number of cpus to assign a new VM', type:Integer),
    Option.new('--ram=RAM', 'Amount of ram to assign a new VM', type:Integer),
    Option.new('--box=BOX', 'Box version to use for deployment', type:String),
  ], examples: "Specific version: ./#{app} deploy --box=0.1.133\n" +
    "Custom nodes: ./#{app} deploy --ips=10,11,12 --cpu=2 --ram=4096")

#  cmdr.add('cluster', 'Kubernetes cluster control', nodes:[
#    Option.new('--init', 'Initialize the main k8s node'),
#    Option.new('--join', 'Join all the slave nodes to the cluster'),
#    Option.new('--dashboard', 'Install dashboard'),
#    Option.new('--helm', 'Install helm')
    # Option.new('--registry=REGISTRY', 'Authorize the given private registry', type:String),
    #"Private registry: ./#{app} deploy --registry=http://example.registry.com")
#  ])
  cmdr.parse!
  begin
    if cmdr[:deploy]
      kubinator.deploy(nodes:cmdr[:deploy][:nodes], cpu:cmdr[:deploy][:cpu], ram:cmdr[:deploy][:ram], box:cmdr[:deploy][:box])
    end

    # Cluster kubinator nodes
#    if opts[:cluster]
#      ips = kubinator.getnodeips()
#      kubinator.deploy(ips, init: opts[:init], join: opts[:join], dashboard: opts[:dashboard], helm: opts[:helm])
#    end
  rescue Interrupt
    puts("\n:: Exiting...".colorize(:light_yellow))
  end
end

# vim: ft=ruby:ts=2:sw=2:sts=2
