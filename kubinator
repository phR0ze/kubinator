#!/usr/bin/env ruby
#MIT License
#Copyright (c) 2017-2018 phR0ze
#
#Permission is hereby granted, free of charge, to any person obtaining a copy
#of this software and associated documentation files (the "Software"), to deal
#in the Software without restriction, including without limitation the rights
#to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
#copies of the Software, and to permit persons to whom the Software is
#furnished to do so, subject to the following conditions:
#
#The above copyright notice and this permission notice shall be included in all
#copies or substantial portions of the Software.
#
#THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
#IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
#FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
#AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
#LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
#OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
#SOFTWARE.

require 'fileutils'
require 'open3'
require 'ostruct'
require 'rubygems/package'
require 'yaml'

# Gems that need to be installed
begin
  require 'nub'
  require 'net/ssh'
  require 'net/scp'
rescue Exception => e
  puts("Error: missing package '#{e.message.split(' ').last.sub('/', '-')}'")
  !puts("Error: install missing packages with 'bundle install --system") and exit
end

class Kubinator
  Node = Struct.new(:name, :ip)

  # Initialize
  # ==== Attributes
  def initialize()

    # Minimum versions
    @k8sver = '1.10.5'
    @helmver = '2.9.1'
    @vagrantver = '1.8.1'
    @virtualboxver = '5.0.32'

    # Semi-static variables
    @user = 'vagrant'
    @pass = 'vagrant'
    @host = 'k8snode'
    @netname = 'vboxnet0'
    @netip = '192.168.56.1'
    @netmask = '255.255.255.0'
    @box = 'phR0ze/cyberlinux-k8snode'
  end

  # Deploy vagrant node/s
  # @param nodes [Array] of last octet ips for vms
  # @param cpu [Int] count to use for vms
  # @param ram [Int] amount of ram in mb for vms
  def deploy(nodes:nil, cpu:nil, ram:nil)
    puts(":: Deploying vagrant vms...".colorize(:light_yellow))

    # Validate vagrant environment
    #---------------------------------------------------------------------------
    puts(":: Validating environment tooling".colorize(:cyan))

    # Check that vagrant is installed and the correct version
    Log.die("Ensure 'vagrant' is installed and on the $PATH") unless FileUtils.exec?('vagrant')
    vagrantver = `vagrant --version`[/\d+\.\d+\.\d+/]
    !puts("Vagrant needs to be version #{@vagrantver} or higher".colorize(:red)) and
      exit unless Gem::Version.new(vagrantver) >= Gem::Version.new(@vagrantver)
    puts("Found Vagrant: #{vagrantver}".colorize(:green))

    # Check that vagrant/virtualbox are on the path and clean previously deployed VMs
    Log.die("Ensure 'virtualbox' is installed and on the $PATH") unless FileUtils.exec?('vboxmanage')
    virtualboxver = `vboxmanage --version`[/\d+\.\d+\.\d+/]
    !puts("Virtualbox needs to be version #{@virtualboxver} or higher".colorize(:red)) and
      exit unless Gem::Version.new(virtualboxver) >= Gem::Version.new(@virtualboxver)
    puts("Found Virtualbox: #{virtualboxver}".colorize(:green))

    # Ensure host-only network exists
    #---------------------------------------------------------------------------
    puts(":: Ensure host-only networking exists".colorize(:cyan))
    config_network = "vboxmanage hostonlyif ipconfig #{@netname} -ip #{@netip} -netmask #{@netmask}"
    if not Sys.exec(config_network, die:false)
      Sys.exec("vboxmanage hostonlyif create")
      Sys.exec(config_network)
    end

    # Generate vagrant node parameters
    #---------------------------------------------------------------------------
    specs = []
    (nodes || (10..12)).each{|node|
      spec = {
        host: "#{@host}#{node}",
        ip: "#{@netip[0..-2]}#{node}/24",
        cpus: cpu || 2,
        ram: ram || 2048,
        vram: 8,
        net: @netname,
        v3d: 'off',
        proxy: Net.proxy.http,
        no_proxy: Net.proxy.no,
        ipv6: nil
      }
      specs << spec
      puts("Generating node: #{spec.to_s}".colorize(:cyan))
    }

    # Read in the template file and write out with ips
    vars = OpenStruct.new
    vars.box = @box
    vars.nodes = specs.map{|x| '  ' + x.to_s} * ",\n"
    FileUtils.cp('vagrant.tpl', 'Vagrantfile')
    FileUtils.resolve('Vagrantfile', vars)

    # Choose a vagrant box to use and update registry if needed
    boxes = Dir[File.join(Dir.pwd, "*.box")]
    if boxes.any?
      puts("Updating vagrant registry for #{boxes.first.colorize(:cyan)}")
      Sys.exec("vagrant box add #{@box} #{boxes.first} --force")
    else
      puts("Downloading #{@box.colorize(:cyan)} from vagrantup.com")
    end

    # Initialize vagrant box
    #-----------------------------------------------------------------------
    puts(":: Initializing vagrant box/s".colorize(:cyan))
    Sys.exec("vagrant up")
    Sys.exec("vagrant reload")
  end

  # Create a snapshot of the vms in the cluster
  # @param cmd [String] save or restor
  def snap(cmd)
    nodes = getnodes.map{|x| x.name}

    if cmd == "save"
      puts(":: Creating snapshot for VMs [#{nodes * ','}]...".colorize(:light_yellow))
    end
  end

  # Create Kubernetes cluster from given nodes
  # @param cmd [String] to execute against the cluster
  # @param registry [String] to authorize
  def cluster(cmd, registry:nil)
    puts(":: Deploying Kubernetes Cluster...".colorize(:light_yellow))
    ips = getnodes.map{|x| x.ip}

    token = 'c74b2e.897db207c8e26de2'

    # Validate host environment
    #---------------------------------------------------------------------------
    puts(":: Validating the K8s host environment".colorize(:cyan))

    # Validate that kubectl is installed on the path and a supported version
    Log.die("Ensure 'kubectl' is installed and on the $PATH") unless FileUtils.exec?('kubectl')
    host_k8sver = `kubectl version`[/GitVersion.*v([\d]+\.[\d]+\.[\d]+)/, 1]
    !puts("Kubectl needs to be version #{@k8sver} or higher".colorize(:red)) and
      exit unless Gem::Version.new(host_k8sver) >= Gem::Version.new(@k8sver)
    puts("Host K8s Version: #{host_k8sver}".colorize(:green))

    # Validate that helm is installed on the path and a supported version
    Log.die("Ensure 'helm' is installed and on the path") unless FileUtils.exec?('helm')
    host_helmver = `helm version --client`[/\d+\.\d+\.\d+/]
    !puts("Helm needs to be version #{@helmver} or higher".colorize(:red)) and
      exit unless Gem::Version.new(host_helmver) >= Gem::Version.new(@helmver)
    puts("Host Helm Version: #{host_helmver}".colorize(:green))

    # Validate node environment
    #---------------------------------------------------------------------------
    puts(":: Validating the K8s node environment".colorize(:cyan))
    node_k8sver = nil
    puts("Nodes: #{ips * ', '}".colorize(:cyan))
    Net::SSH.start(ips.first, @user, password:@pass, verify_host_key: :never){|ssh|
      node_k8sver = ssh.exec!("kubectl version")[/GitVersion.*v([\d]+\.[\d]+\.[\d]+)/, 1]
      !puts("Node K8s needs to be version #{@k8sver} or higher".colorize(:red)) and
        exit unless Gem::Version.new(node_k8sver) >= Gem::Version.new(@k8sver)
      puts("Node K8s Version: #{node_k8s_ver}".colorize(:cyan))

      node_helmver = ssh.exec!("helm version --client")[/SemVer.*v([\d]+\.[\d]+\.[\d]+)/, 1]
      !puts("Node Helm needs to be version #{@helmver} or higher".colorize(:red)) and
        exit unless Gem::Version.new(node_helmver) >= Gem::Version.new(@helmver)
      puts("Node Helm Version: #{node_helm_ver}".colorize(:cyan))
    }
    exit

    #!puts("Please unset KUBECONFIG. We will be using 'contexts' instead".colorize(:red)) and
    #  exit unless not ENV['KUBECONFIG']

    # Ensure vm ips are in the $no_proxy
    #proxy_export
    #!puts("Ensure 'no_proxy' includes your node ips #{ips * ','}".colorize(:red)) and
    #  exit unless not @proxy or ips.each{|x| ENV['no_proxy'].include?(x)}
    #puts("$no_proxy configured correctly".colorize(:green)) if @proxy

    # Clear K8s cache
    #---------------------------------------------------------------------------
    cache_dir = File.expand_path("~/.kube")
    FileUtils.rm_rf(cache_dir)
    FileUtils.mkdir_p(cache_dir)

    # (Idempotent) Configure nodes for clustering
    #---------------------------------------------------------------------------
    threads = []
    ips.each{|ip| threads << Thread.new{
      Net::SSH.start(ip, @user, password:@pass, paranoid:false) do |ssh|

        # Configure journald for persistent storage
        journald_conf = '/etc/systemd/journald.conf'
        if not ssh.exec!("cat #{journald_conf}").include?('persistent')
          ssh.exec!("sudo sed -i -e 's/.*\\(Storage=\\).*/\\1persistent/' #{journald_conf}")
          ssh.exec!("sudo systemctl restart systemd-journald")
          puts("#{ip}: Configured node journald...done".colorize(:cyan))
        else
          puts("#{ip}: Configured node journald...skipped".colorize(:cyan))
        end

        # Configure kubelet/k8s api server
        # https://kubernetes.io/docs/admin/kubeadm
        kubeadm_conf = '/etc/systemd/system/kubelet.service.d/10-kubeadm.conf'
        if not ssh.exec!("cat #{kubeadm_conf}").include?(ip)
          api_args = [
            "--node-ip=#{ip}",
            "--hostname-override=#{ip}"
            # Else: /etc/kubernetes/manifests/kube-apiserver.yaml
            # - --runtime-config=batch/v2alpha1=true
            # To enable jobs: kubeadmin init --config /etc/kubernetes/kubeadm.conf
            #"--runtime-config=batch\\/v2alpha1=true"
          ]
          extra_args = "Environment=\"KUBELET_EXTRA_ARGS=#{api_args * ' '}\"\\n"
          ssh.exec!("sudo sed -i -e 's/\\(ExecStart=\\)$/#{extra_args}\\1/' #{kubeadm_conf}")
          ssh.exec!("sudo systemctl daemon-reload")
          ssh.exec!("sudo systemctl restart kubelet")
          puts("#{ip}: Configured Kubelet private network ip...done".colorize(:cyan))
        else
          puts("#{ip}: Configured Kubelet private network ip...skipped".colorize(:cyan))
        end

        # Configure kernel for Elasticsearch
        sysctl_conf = '/etc/sysctl.d/10-cyberlinux.conf'
        if not ssh.exec!("cat #{sysctl_conf}").include?('max_map_count')
          ssh.exec!("sudo bash -c 'echo \"vm.max_map_count = 262144\" >> #{sysctl_conf}'")
          ssh.exec!("sudo sysctl -w vm.max_map_count=262144")
        else
          puts("#{ip}: Configured kernel params...skipped".colorize(:cyan))
        end

        # Set docker storage-driver from overlay2 to overlay
        driver = "overlay"
        daemon_conf = '/etc/docker/daemon.json'
        if not ssh.exec!("[ -e #{daemon_conf} ] && echo 'exists'").include?('exists')

          # Configure docker storage-driver
          docker_conf = "{\\\"storage-driver\\\":\\\"#{driver}\\\"}"
          ssh.exec!("sudo bash -c 'mkdir -p /etc/docker/'")
          ssh.exec!("sudo bash -c 'echo \"#{docker_conf}\" > /etc/docker/daemon.json'")

          ssh.exec!("sudo systemctl daemon-reload")
          ssh.exec!("sudo systemctl restart docker")
          puts("#{ip}: Configured Docker storage-driver...done".colorize(:cyan))
        else
          puts("#{ip}: Configured Docker storage-driver...skipped".colorize(:cyan))
        end

        # Optionally - configure private registry
        if registry
          docker_opts = [ "--registry-mirror=http://#{registry}", "--insecure-registry #{registry}" ]
          override_conf = '/etc/systemd/system/docker.service.d/override.conf'
          if not ssh.exec!("[ -e #{override_conf} ] && echo 'exists'").include?('exists')

            # Configure kubernetes for private registry
            docker_conf = "{\\\"auths\\\":{\\\"#{registry}\\\":{\\\"auth\\\":\\\"YW5vbnltb3VzOmFub255bW91cw==\\\"}}}"
            ssh.exec!("sudo bash -c 'mkdir -p /root/.docker'")
            ssh.exec!("sudo bash -c 'echo \"#{docker_conf}\" > /root/.docker/config.json'")

            # Configure docker for private registry
            ssh.exec!("sudo bash -c 'echo \"[Service]\" > #{override_conf}'")
            ssh.exec!("sudo bash -c 'echo \"ExecStart=\" >> #{override_conf}'")
            ssh.exec!("sudo bash -c 'echo \"ExecStart=/usr/bin/dockerd #{docker_opts * ' '} -H fd://\" >> #{override_conf}'")
            ssh.exec!("sudo systemctl daemon-reload")
            ssh.exec!("sudo systemctl restart docker")
            puts("#{ip}: Configured Docker overrides...done".colorize(:cyan))
          else
            puts("#{ip}: Configured Docker overrides...skipped".colorize(:cyan))
          end

        end
      end
    }}
    threads.each{|x| x.join}

    # (Idempotent) Initialize master node
    # https://kubernetes.io/docs/getting-started-guides/kubeadm
    #---------------------------------------------------------------------------
    if all or init
      Net::SSH.start(ips.first, @user, password:@pass, paranoid:false) do |ssh|
        if not ssh.exec!("docker ps").include?("google")

          # Initialize master - pod-network injects the --cluster-cidr into kube-proxy
          puts("#{ips.first}: Initialize Master node...".colorize(:cyan))
          cmd = "sudo kubeadm init --token=#{token} --kubernetes-version=v#{k8sver} --apiserver-advertise-address=#{ips.first}"
          ssh.exec!(cmd){|c,s,o|puts(o)}
          (1..10).each{|i| puts("Waiting for k8s networking to quiesce..#{i}"); sleep(1)}
          ssh.exec!("mkdir -p ~/.kube")
          ssh.exec!("sudo mkdir -p /root/.kube")
          ssh.exec!("sudo cp /etc/kubernetes/admin.conf /root/.kube/config")
          ssh.exec!("sudo cp /etc/kubernetes/admin.conf ~/.kube/config")
          ssh.exec!("sudo chown $(id -u):$(id -g) ~/.kube/config")
          ssh.exec!("kubectl config use-context kubernetes-admin@kubernetes")
          ssh.exec!("sudo kubectl config use-context kubernetes-admin@kubernetes")

          # Disable RBAC for now (will wait until needed)
          # Bottom of https://kubernetes.io/docs/admin/authorization/rbac
          ssh.exec!("kubectl create clusterrolebinding permissive-binding --clusterrole=cluster-admin --user=admin --user=kubelet --group=system:serviceaccounts"){|c,s,o|puts(o)}

          # Patch kube-proxy for dns resolution using --proxy-mode=iptables
          # https://github.com/kubernetes/kubernetes/issues/18934
          get_kube_proxy = "kubectl -n kube-system get ds kube-proxy -o json"
          mod_kube_proxy = " | jq '.spec.template.spec.containers[0].command |= .+ [\"--proxy-mode=iptables\"]'"
          mod_kube_proxy += " | jq '.spec.template.spec.containers[0].command |= .+ [\"--cluster-cidr=10.32.0.0/12\"]'"
          set_kube_proxy = " | kubectl apply -f -"
          ssh.exec!("#{get_kube_proxy} #{mod_kube_proxy} #{set_kube_proxy}"){|c,s,o|puts(o)}
          ssh.exec!("kubectl -n kube-system delete po -l 'k8s-app=kube-proxy'"){|c,s,o|puts(o)}

          # Taint master node to allow pods to be scheduled on it
          # Check current taints in Tains section: kubectl describe node
          # Note: this needs to be done before deploying flannel
          puts("Tainting the master node to allow pods to be scheduled on it".colorize(:cyan))
          ssh.exec!("kubectl taint nodes --all node-role.kubernetes.io/master-"){|c,s,o|puts(o)}

          # Install Pod networking
          # Note: kube-dns will be pending until the pod networking is installed
          puts("Installing weave networking".colorize(:cyan))
          ssh.exec!("#{@proxy_export}kubectl apply -f https://git.io/weave-kube-1.6"){|c,s,o|puts(o)}
        end
      end
    end

    # (Idempotent) Configure slaves to join cluster
    #---------------------------------------------------------------------------
    if all or join
      nodes_to_join = []
      Net::SSH.start(ips.first, @user, password:@pass, paranoid:false) do |ssh|
        output = getpods(ssh:ssh)
        nodes_to_join = ips.select{|x| !output.include?(x)}
      end
      nodes_to_join.each do |ip|
        Net::SSH.start(ip, @user, password:@pass, paranoid:false) do |ssh|
          puts("#{ip}: Joining cluster".colorize(:cyan))
          ssh.exec!("sudo kubeadm join --token=#{token} #{ips.first}:6443"){|c,s,o|puts(o)}
        end
      end
    end

    # (Idempotent) Install dashboard and helm
    #---------------------------------------------------------------------------
    Net::SSH.start(ips.first, @user, password:@pass, paranoid:false) do |ssh|

      # Install dashboard
      if all or dashboard
        if not getpods(pod:'dashboard', ssh:ssh)
          puts("#{ips.first}: Installing dashboard".colorize(:cyan))
          url = "https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml"
          puts(ssh.exec!("kubectl create -f #{url}"))
        else
          puts("#{ips.first}: Installing dashboard...skipped".colorize(:cyan))
        end
      end

      # Initialize/update helm
      if all or helm
        if not getpods(pod:'tiller', ssh:ssh)
          puts("#{ips.first}: Initializing helm".colorize(:cyan))
          ssh.exec!("#{@proxy_export}helm init"){|c,s,o|puts(o)}
          ssh.exec!("#{@proxy_export}helm repo update"){|c,s,o|puts(o)}
        else
          puts("#{ips.first}: Initializing/updating helm...skipped".colorize(:cyan))
        end
      end
    end

    # Configure local kubectl
    kube_conf = File.join(cache_dir, "config")
    FileUtils.rm_rf(kube_conf)
    Net::SCP.download!(ips.first, @user, '.kube/config', kube_conf, ssh:{paranoid: false, password: @pass})
    Sys.exec("kubectl config use-context kubernetes-admin@kubernetes")
    puts("#{'-' * 80}".colorize(:cyan))
    Sys.exec("kubectl get po --all-namespaces -o wide")
    puts("#{'-' * 80}".colorize(:cyan))
    puts("Your cluster will need a few minutes to be fully operational".colorize(:cyan))
  end

  # Wait for the given pod to be ready
  # Blocks until service is ready
  # ==== Attributes
  # * +pod+ - pod by name to wait for
  def podready!(pod)
    details = getpods(pod:pod)
    status = details ? details[3] : ""

    # Skip wait if pod already running
    if status.include?('Running')
      puts("Waiting for '#{pod}' to be ready - Running".colorize(:cyan))

    # Wait for pod to be ready
    else
      ready = 0
      until ready > 1
        !puts("Waiting for '#{pod}' to be ready - #{status}".colorize(:cyan)) and sleep(10)
        details = getpods(pod:pod)
        status = details ? details[3] : ""

        ready += 1 if status.include?('Running')
      end
    end
  end

  # Destroy the vagrant vms
  def destroy
    getnodes.each{|node|
      Sys.exec("vagrant destroy #{node.name} -f")
    }
  end

  # Get pod details
  # ==== Attributes
  # * +pod+ - pod by name to pull details for
  # * +ssh+ - optionaly use ssh connection to use
  def getpods(pod:nil, ssh:nil)
    details = ssh ? ssh.exec!("kubectl get pod --all-namespaces -o wide") :
      `kubectl get pod --all-namespaces -o wide`
    if pod
      details = details.split("\n").find{|x| x.include?(pod)}
      details = details.split(' ').map{|x| x.strip} if details
      return details
    end
    return details
  end

  # Parse the node names out of the Vagrantfile
  # @returns [Array(Node)] list of nodes
  def getnodes
    nodes = []

    pattern = @netip.split('.')[0..-2] * '.'
    File.open('Vagrantfile', 'r'){|f|
      f.readlines.each{|x|
        if x =~ /#{pattern}.*/
          nodes << Node.new(x[/k8snode[\d]+/], x[/#{pattern}\.\d+/])
        end
      }
    }

    return nodes
  end
end

#-------------------------------------------------------------------------------
# Main entry point
#-------------------------------------------------------------------------------
if __FILE__ == $0
  app = 'kubinator'
  kubinator = Kubinator.new
  version = FileUtils.version('.gemspec', /\s*spec\.version\s*=.*(\d+\.\d+\.\d+).*/)
  examples = "Deploy: ./#{app} deploy\n".colorize(:green)
  examples += "Deploy custom: ./#{app} deploy --ips=10,11,12 --cpu=2 --ram=4096\n".colorize(:green)
  cmdr = Commander.new(app:app, version:version, examples:examples)

  # Deploy vms
  #-----------------------------------------------------------------------------
  cmdr.add('deploy', 'Deploy Vagrant nodes', nodes:[
    Option.new('--nodes=NODES', 'List of last octet IPs (e.g. 10,11,2)', type:Array),
    Option.new('--cpu=CPU', 'Number of cpus to assign a new VM', type:Integer),
    Option.new('--ram=RAM', 'Amount of ram to assign a new VM', type:Integer),
  ], examples: "Standard: ./#{app} deploy\n" +
    "Custom nodes: ./#{app} deploy --ips=10,11,12 --cpu=2 --ram=4096")

  # Manage vm snapshots
  #-----------------------------------------------------------------------------
  cmdr.add('snap', 'Manage VM snapshots', nodes:[
    Option.new(nil, 'Snapshot command to execute', allowed:{
      save: "Create a new snapshot",
      restore: "Restore previous snapshot",
    }),
  ], examples: "Create snapshot: ./#{app} snap save\n" + 
    "Restore snapshot: ./#{app} snap restore\n")

  # Destroy vms
  #-----------------------------------------------------------------------------
  cmdr.add('destroy', 'Destroy Vagrant nodes', nodes:[
  ], examples: "Destroy nodes: ./#{app} destroy\n")

  # Manage K8s cluster
  #-----------------------------------------------------------------------------
  cmdr.add('cluster', 'Kubernetes cluster control', nodes:[
    Option.new(nil, 'Cluster command', required:true, allowed:{
      init: 'Initialize the Kubernetes cluster'
    }),
  ], examples: "Initialize cluster: ./#{app} cluster init\n")
    # Option.new('--registry=REGISTRY', 'Authorize the given private registry', type:String),
    #"Private registry: ./#{app} deploy --registry=http://example.registry.com")

  cmdr.parse!
  begin
    if cmdr[:deploy]
      kubinator.deploy(nodes:cmdr[:deploy][:nodes], cpu:cmdr[:deploy][:cpu], ram:cmdr[:deploy][:ram])
    end
    if cmdr[:snap]
      kubinator.snap(cmdr[:snap][:snap0])
    end
    if cmdr[:destroy]
      kubinator.destroy
    end
    if cmdr[:cluster]
      kubinator.cluster(cmdr[:cluster][:cluster0])
    end
  rescue Interrupt
    puts("\n:: Exiting...".colorize(:light_yellow))
  end
end

# vim: ft=ruby:ts=2:sw=2:sts=2
